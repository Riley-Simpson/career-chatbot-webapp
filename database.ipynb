{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install Flask Flask-Cors llama-index sentence-transformers chromadb requests numpy scikit-learn pandas python-docx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from flask import Flask, request, jsonify\n",
    "from llama_index.core.readers.json import JSONReader\n",
    "from llama_index import SimpleDirectoryReader\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "from flask_cors import CORS\n",
    "\n",
    "app = Flask(__name__)\n",
    "CORS(app)\n",
    "\n",
    "# Initialize the SentenceTransformer model\n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "# Initialize Chroma DB\n",
    "client = chromadb.Client()\n",
    "collection = client.create_collection('resume_data')\n",
    "\n",
    "# Predefined directory path in the Colab environment\n",
    "PREDEFINED_DIRECTORY_PATH = \"/content/drive/MyDrive/Dissertation_Database\"\n",
    "\n",
    "def load_data(directory_path):\n",
    "    try:\n",
    "        # Clear existing collection\n",
    "        collection.clear()\n",
    "\n",
    "        # Load and process files\n",
    "        for root, _, files in os.walk(directory_path):\n",
    "            for file in files:\n",
    "                file_path = os.path.join(root, file)\n",
    "                if file.endswith('.json'):\n",
    "                    reader = JSONReader(\n",
    "                        levels_back=0,\n",
    "                        collapse_length=200,\n",
    "                        ensure_ascii=False,\n",
    "                        is_jsonl=False,\n",
    "                        clean_json=True,\n",
    "                    )\n",
    "                    documents = reader.load_data(input_file=file_path, extra_info={})\n",
    "                else:\n",
    "                    reader = SimpleDirectoryReader(root)\n",
    "                    documents = reader.load_data()\n",
    "\n",
    "                for idx, doc in enumerate(documents):\n",
    "                    doc_text = doc.get_text()\n",
    "                    doc_embedding = model.encode([doc_text])\n",
    "                    collection.add(doc_embedding, metadata={'id': idx, 'doc': doc_text})\n",
    "\n",
    "        return {\"message\": \"Dataset Loaded\"}, 200\n",
    "    except Exception as e:\n",
    "        return {\"error\": f\"Error loading data: {e}\"}, 500\n",
    "\n",
    "@app.route(\"/load_data\", methods=[\"POST\"])\n",
    "def load_data_endpoint():\n",
    "    return jsonify(*load_data(PREDEFINED_DIRECTORY_PATH))\n",
    "\n",
    "@app.route(\"/retrieve_documents\", methods=[\"POST\"])\n",
    "def retrieve_documents():\n",
    "    data = request.get_json()\n",
    "    query_str = data[\"query_str\"]\n",
    "    top_k = data.get(\"top_k\", 3)\n",
    "\n",
    "    try:\n",
    "        query_embedding = model.encode([query_str])\n",
    "        similarities = collection.similarity_search(query_embedding, top_k=top_k)\n",
    "\n",
    "        retrieved_docs = [sim['metadata']['doc'] for sim in similarities]\n",
    "        return jsonify({\"documents\": retrieved_docs}), 200\n",
    "\n",
    "    except Exception as e:\n",
    "        return jsonify({\"error\": f\"Error retrieving documents: {e}\"}), 500\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run(host=\"0.0.0.0\", port=5001)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
