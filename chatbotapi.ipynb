{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirments.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, ServiceContext\n",
    "from llama_index.core.readers.json import JSONReader\n",
    "from llama_index.llms.huggingface import HuggingFaceLLM\n",
    "from llama_index.core.prompts.prompts import SimpleInputPrompt\n",
    "from huggingface_hub import login\n",
    "import json ,torch\n",
    "from flask import Flask, request, jsonify\n",
    "from flask_ngrok import run_with_ngrok\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from llama_index.core import ServiceContext\n",
    "from llama_index.legacy.embeddings.langchain import LangchainEmbedding\n",
    "\n",
    "class chat:\n",
    "    def __init__(self):\n",
    "        self.dataset = None\n",
    "        self.llm = None\n",
    "        self.main\n",
    "\n",
    "    def hg_login(self):\n",
    "        with open(\"HUGGING_FACE_TOKEN.json\", \"r\") as f:\n",
    "            api_key = json.load(f)[\"access_token\"]\n",
    "        login(token=api_key)\n",
    "        return print(\"Succesfully Logged in.\")\n",
    "    \n",
    "    def data_load(self):\n",
    "        reader = JSONReader(\n",
    "            levels_back=0,\n",
    "            collapse_length=200,\n",
    "            ensure_ascii=False,\n",
    "            is_jsonl=False,\n",
    "            clean_json=True,\n",
    "            )\n",
    "        self.dataset = reader.load_data(input_file=\"resume_data.json\", extra_info={})\n",
    "        print(\"Dataset Loaded\")\n",
    "    \n",
    "    def load_model(self):\n",
    "        system_prompt=\"\"\"\n",
    "        You are a Career Advisor for students at the University of Strathclyde.\n",
    "        Your job is to advice students asaccurately as possible based on the data being provided.\n",
    "        \"\"\"\n",
    "        query_wrapper_prompt=SimpleInputPrompt(\"<|USER|>{query_str}<|ASSISTANT|>\")\n",
    "        self.llm = HuggingFaceLLM(\n",
    "            context_window=4096,\n",
    "            max_new_tokens=256,\n",
    "            generate_kwargs={\"temperature\": 0.0, \"do_sample\": False},\n",
    "            system_prompt=system_prompt,\n",
    "            query_wrapper_prompt=query_wrapper_prompt,\n",
    "            tokenizer_name=\"meta-llama/Llama-2-7b-chat-hf\",\n",
    "            model_name=\"meta-llama/Llama-2-7b-chat-hf\",\n",
    "            device_map=\"auto\",\n",
    "            # uncomment this if using CUDA to reduce memory usage\n",
    "            model_kwargs={\"torch_dtype\": torch.float16 , \"load_in_8bit\":True}\n",
    "        )\n",
    "        print(\"LLM Loaded\")\n",
    "        \n",
    "    \n",
    "    def main(self):\n",
    "        self.hg_login()\n",
    "        self.data_load()\n",
    "        self.load_model()\n",
    "\n",
    "chat_instance = chat()\n",
    "chat_instance.main()\n",
    "\n",
    "\n",
    "app = Flask(__name__)\n",
    "run_with_ngrok(app)  # Start ngrok when app is run\n",
    "\n",
    "@app.route(\"/query\", methods=[\"POST\"])\n",
    "def query():\n",
    "    data = request.get_json()\n",
    "    response = chat_instance.query(data[\"query\"])\n",
    "    return jsonify({\"response\": response})\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
