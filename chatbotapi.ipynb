{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirments.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, ServiceContext\n",
    "from llama_index.core.readers.json import JSONReader\n",
    "from llama_index.llms.huggingface import HuggingFaceLLM\n",
    "from llama_index.core.prompts.prompts import SimpleInputPrompt\n",
    "from huggingface_hub import login\n",
    "from flask import Flask, request, jsonify\n",
    "from pyngrok import ngrok\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from llama_index.legacy.embeddings.langchain import LangchainEmbedding\n",
    "\n",
    "\n",
    "\n",
    "class Chat:\n",
    "    def __init__(self):\n",
    "        self.dataset = None\n",
    "        self.llm = None\n",
    "        self.service_context = None\n",
    "        self.index=None\n",
    "        self.query_engine=None\n",
    "        \n",
    "\n",
    "    def hg_login(self):\n",
    "        with open(\"HUGGING_FACE_TOKEN.json\", \"r\") as f:\n",
    "            api_key = json.load(f)[\"access_token\"]\n",
    "        login(token=api_key)\n",
    "        print(\"Successfully Logged in.\")\n",
    "    \n",
    "    def data_load(self):\n",
    "        reader = JSONReader(\n",
    "            levels_back=0,\n",
    "            collapse_length=200,\n",
    "            ensure_ascii=False,\n",
    "            is_jsonl=False,\n",
    "            clean_json=True,\n",
    "        )\n",
    "        self.dataset = reader.load_data(input_file=\"resume_data.json\", extra_info={})\n",
    "        print(\"Dataset Loaded\")\n",
    "    \n",
    "    def load_model(self):\n",
    "        system_prompt = \"\"\"\n",
    "        You are a Career Advisor for students at the University of Strathclyde.\n",
    "        Your job is to advise students as accurately as possible based on the data being provided.\n",
    "        \"\"\"\n",
    "        query_wrapper_prompt = SimpleInputPrompt(\"{query_str}\")\n",
    "        self.llm = HuggingFaceLLM(\n",
    "            context_window=4096,\n",
    "            max_new_tokens=256,\n",
    "            generate_kwargs={\"temperature\": 0.0, \"do_sample\": False},\n",
    "            system_prompt=system_prompt,\n",
    "            query_wrapper_prompt=query_wrapper_prompt,\n",
    "            tokenizer_name=\"meta-llama/Llama-2-7b-chat-hf\",\n",
    "            model_name=\"meta-llama/Llama-2-7b-chat-hf\",\n",
    "            device_map=\"auto\",\n",
    "            model_kwargs={\"torch_dtype\": torch.float16, \"load_in_8bit\": True}\n",
    "        )\n",
    "        print(\"LLM Loaded\")\n",
    "    \n",
    "    def embed_model(self):\n",
    "        self.embedded_model=LangchainEmbedding(HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\"))\n",
    "        return print(\"Model Embeded\")\n",
    "\n",
    "    def service_context(self):\n",
    "        self.service_context=ServiceContext.from_defaults(\n",
    "        chunk_size=1024,\n",
    "        llm=self.llm,\n",
    "        embed_model=self.embed_model)\n",
    "        print(\"Service Context established\")\n",
    "\n",
    "    def engine_start(self):\n",
    "        self.index = VectorStoreIndex.from_documents(self.data,service_context=self.service_context)\n",
    "        self.query_engine= self.index.as_query_engine()\n",
    "        print(\"Engine Loaded\")\n",
    "\n",
    "    def query(self, query_str):\n",
    "        return self.query_engine(query_str)\n",
    "\n",
    "    def initialize(self):\n",
    "        self.hg_login()\n",
    "        self.data_load()\n",
    "        self.load_model()\n",
    "        self.embed_model()\n",
    "        self.service_context()\n",
    "        self.engine_start()\n",
    "\n",
    "\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route(\"/query\", methods=[\"POST\"])\n",
    "def query():\n",
    "    chat_instance = Chat()\n",
    "    chat_instance.initialize()\n",
    "    data = request.get_json()\n",
    "    response = chat_instance.query(data[\"query\"])\n",
    "    return jsonify({\"response\": response})\n",
    "\n",
    "\n",
    "with open(\"ngrok.json\",\"r\") as f:\n",
    "    ngrok_api_key = json.load(f)[\"authtoken\"]\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    ngrok.set_auth_token(ngrok_api_key)\n",
    "    public_url = ngrok.connect(5000)\n",
    "    print(f\" * ngrok tunnel \\\"{public_url}\\\" -> \\\"http://0.0.0.0:5000\\\"\")\n",
    "    app.run(host=\"0.0.0.0\",port=5000)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
